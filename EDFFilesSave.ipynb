{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "import os\n",
    "\n",
    "def get_links_from_url(url, base_url, visited, current_depth):\n",
    "    \"\"\"\n",
    "    Function to retrieve all 'href' links from a given URL, filtering out query parameters, parent directory links,\n",
    "    and ensuring we only navigate to unvisited subdirectories that go deeper in the hierarchy.\n",
    "    \"\"\"\n",
    "    response = requests.get(url, auth=(username, password))  # Add auth if needed\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        links = []\n",
    "        \n",
    "        # Find all anchor tags with href attributes\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            \n",
    "            # Filter out links that move up a level (like \"../\") or are query parameters\n",
    "            if not href.startswith('?') and href.endswith('/') and not href.startswith('../'):\n",
    "                full_url = urllib.parse.urljoin(url, href)\n",
    "                \n",
    "                # Ensure we're only navigating to deeper directories (by checking path depth)\n",
    "                if full_url.startswith(base_url) and full_url != url and full_url not in visited:\n",
    "                    # Check if the new URL is deeper in the directory structure\n",
    "                    new_depth = full_url.count('/')\n",
    "                    if new_depth > current_depth:  # Only add if it is deeper\n",
    "                        links.append(full_url)\n",
    "        \n",
    "        return links\n",
    "    else:\n",
    "        print(f\"Failed to access {url}. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def download_csv_files(url, base_url, download_dir, username=None, password=None):\n",
    "    \"\"\"\n",
    "    Function to download all EDF files from a given URL and save them to a specified directory.\n",
    "    \"\"\"\n",
    "    response = requests.get(url, auth=(username, password))  # Add auth if needed\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Create the directory if it doesn't exist\n",
    "        if not os.path.exists(download_dir):\n",
    "            os.makedirs(download_dir)\n",
    "        \n",
    "        # Find all anchor tags with href attributes pointing to CSV files\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            \n",
    "            # If it's an EDF file, download it\n",
    "            if href.endswith('.edf'):\n",
    "                file_url = urllib.parse.urljoin(url, href)\n",
    "                file_name = os.path.basename(file_url)\n",
    "                file_path = os.path.join(download_dir, file_name)\n",
    "                \n",
    "                # Download the EDF file\n",
    "                edf_response = requests.get(file_url, auth=(username, password))\n",
    "                if edf_response.status_code == 200:\n",
    "                    with open(file_path, 'wb') as file:\n",
    "                        file.write(edf_response.content)\n",
    "                    print(f\"Downloaded: {file_path}\")\n",
    "                else:\n",
    "                    print(f\"Failed to download {file_name}\")\n",
    "    else:\n",
    "        print(f\"Failed to access {url}. Status code: {response.status_code}\")\n",
    "\n",
    "def find_all_third_level_and_download_csvs(base_url, download_dir, username=None, password=None):\n",
    "    \"\"\"\n",
    "    Function to find all third-level directories under all second-level directories under all first-level directories,\n",
    "    and download all EDF files from these third-level directories to a specified folder.\n",
    "    \"\"\"\n",
    "    visited = set()  # To track visited URLs\n",
    "    current_depth = base_url.count('/')  # Track the current depth level\n",
    "    print(f\"Base URL: {base_url}\")\n",
    "    \n",
    "    # Get first-level links\n",
    "    first_level_links = get_links_from_url(base_url, base_url, visited, current_depth)\n",
    "    for first_link in first_level_links:\n",
    "        visited.add(first_link)\n",
    "        current_depth = first_link.count('/')  # Update depth\n",
    "        print(f\"First-level URL: {first_link}\")\n",
    "        \n",
    "        # Get second-level links\n",
    "        second_level_links = get_links_from_url(first_link, base_url, visited, current_depth)\n",
    "        for second_link in second_level_links:\n",
    "            visited.add(second_link)\n",
    "            current_depth = second_link.count('/')  # Update depth\n",
    "            print(f\"Second-level URL: {second_link}\")\n",
    "            \n",
    "            # Get third-level links and download CSV files\n",
    "            third_level_links = get_links_from_url(second_link, base_url, visited, current_depth)\n",
    "            for third_link in third_level_links:\n",
    "                visited.add(third_link)\n",
    "                current_depth = third_link.count('/')  # Update depth\n",
    "                print(f\"Third-level URL: {third_link}\")\n",
    "                \n",
    "                # Now download all EDF files from this third-level directory to the specified folder\n",
    "                download_csv_files(third_link, base_url, download_dir, username, password)\n",
    "\n",
    "# # Example usage:\n",
    "# base_url =  \n",
    "# download_dir =\n",
    "# # Authentication credentials\n",
    "# username = \n",
    "# password \n",
    "\n",
    "find_all_third_level_and_download_csvs(base_url, download_dir, username, password)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eegnetmibci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
