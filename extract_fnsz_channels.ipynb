{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract focal channels\n",
    "def extract_fnsz_channels(file_path):\n",
    "    \"\"\"\n",
    "    Extract channel names corresponding to rows with the label 'fnsz' for each start time,\n",
    "    merging start times that are 5 seconds or less apart into the earliest start time.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are start times and values are lists of channel names.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load CSV data, skipping lines that start with '#'\n",
    "        data = pd.read_csv(file_path, comment='#')\n",
    "        \n",
    "        # Filter rows with the label 'fnsz'\n",
    "        fnsz_data = data[data['label'] == 'fnsz']\n",
    "        \n",
    "        # Helper function to convert differential montage channels to detailed channel names\n",
    "        def convert_channel_to_detailed(channel):\n",
    "            return channel.split('-')  # Split into parts for detailed representation\n",
    "        \n",
    "        # Sort start times and merge them if they are within 5 seconds\n",
    "        sorted_times = sorted(fnsz_data['start_time'].unique())\n",
    "        merged_start_times = []\n",
    "        current_group = [sorted_times[0]]\n",
    "\n",
    "        for i in range(1, len(sorted_times)):\n",
    "            if sorted_times[i] - current_group[-1] <= 5:\n",
    "                current_group.append(sorted_times[i])\n",
    "            else:\n",
    "                merged_start_times.append(current_group)\n",
    "                current_group = [sorted_times[i]]\n",
    "        merged_start_times.append(current_group)  # Add the last group\n",
    "        \n",
    "        # Create a mapping from each original start time to its merged start time\n",
    "        start_time_mapping = {\n",
    "            time: group[0] for group in merged_start_times for time in group\n",
    "        }\n",
    "        \n",
    "        # Map each start time to the merged time\n",
    "        fnsz_data['merged_start_time'] = fnsz_data['start_time'].map(start_time_mapping)\n",
    "        \n",
    "        # Group by 'merged_start_time' and extract unique, detailed channels for each group\n",
    "        focal_zone_channels = (\n",
    "            fnsz_data.groupby('merged_start_time')['channel']\n",
    "            .apply(lambda channels: list(\n",
    "                set(\n",
    "                    detailed_channel\n",
    "                    for ch in channels.unique()\n",
    "                    for detailed_channel in convert_channel_to_detailed(ch)\n",
    "                ) & set(electrodes)  # Filter channels based on the predefined 'electrodes' set\n",
    "            ))\n",
    "            .to_dict()\n",
    "        )\n",
    "        \n",
    "        return focal_zone_channels\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file '{file_path}': {e}\")\n",
    "        return {}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eegnetmibci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
