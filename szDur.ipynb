{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script Name: szDur\n",
    "\n",
    "Author: Fatemeh Delavari  \n",
    "Original Version: (02/07/2025)\n",
    "Version: 2.0 (02/10/2025)  \n",
    "Description: Calculates duration of seizures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Path to the folder containing CSV files\n",
    "folder_path = 'C:/Users/Atena/Documents/csv_files'\n",
    "\n",
    "# Get all CSV file paths in the folder\n",
    "csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "\n",
    "# Define the target labels to check against\n",
    "target_labels = ['bckg', 'seiz', 'fnsz', 'gnsz', 'spsz', 'cpsz', 'absz', \n",
    "                 'tnsz', 'cnsz', 'tcsz', 'atsz', 'mysz', 'nesz']\n",
    "\n",
    "# Initialize dictionaries for each label\n",
    "long_sz = {f'{label}': [] for label in target_labels}\n",
    "\n",
    "# Loop over each CSV file and process it\n",
    "for file_path in csv_files:\n",
    "    # Load the CSV file, skipping the commented lines\n",
    "    df = pd.read_csv(file_path, comment='#')\n",
    "    \n",
    "    # Convert label column to lowercase for case-insensitive matching\n",
    "    df['label'] = df['label'].str.lower()\n",
    "    \n",
    "    # Filter the rows that match the target labels\n",
    "    matching_df = df[df['label'].isin(target_labels)]\n",
    "    \n",
    "    # Calculate the time differences (stop_time - start_time)\n",
    "    matching_df['time_diff'] = matching_df['stop_time'] - matching_df['start_time']\n",
    "    \n",
    "    # Process each label and add the unique sorted values to the corresponding dictionaries\n",
    "    for label in target_labels:\n",
    "        label_data = matching_df[matching_df['label'] == label]\n",
    "        \n",
    "        # Extract and sort unique duration values\n",
    "        unique_dur = sorted(set(label_data['time_diff']))\n",
    "\n",
    "        if any(dur>1800 for dur in unique_dur):\n",
    "        # if sum(unique_dur)>3000:\n",
    "            file_name = \"\".join(file_path)[-22:]\n",
    "            long_sz[f'{label}'].append(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Path to the folder containing CSV files\n",
    "folder_path = 'C:/Users/Atena/Documents/csv_files'\n",
    "\n",
    "# Get all CSV file paths in the folder\n",
    "csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "\n",
    "# Define the target labels to check against\n",
    "target_labels = ['bckg', 'seiz', 'fnsz', 'gnsz', 'spsz', 'cpsz', 'absz', \n",
    "                 'tnsz', 'cnsz', 'tcsz', 'atsz', 'mysz', 'nesz']\n",
    "# target_labels = ['bckg']\n",
    "\n",
    "# Initialize dictionaries for each label\n",
    "long_sz = {f'{label}': [] for label in target_labels}\n",
    "\n",
    "# Loop over each CSV file and process it\n",
    "for file_path in csv_files:\n",
    "    # Load the CSV file, skipping the commented lines\n",
    "    df = pd.read_csv(file_path, comment='#')\n",
    "    \n",
    "    # Convert label column to lowercase for case-insensitive matching\n",
    "    df['label'] = df['label'].str.lower()\n",
    "    \n",
    "    # Filter the rows that match the target labels\n",
    "    matching_df = df[df['label'].isin(target_labels)]\n",
    "    \n",
    "    # Calculate the time differences (stop_time - start_time)\n",
    "    matching_df['time_diff'] = matching_df['stop_time'] - matching_df['start_time']\n",
    "    \n",
    "    # Process each label and add the unique sorted values to the corresponding dictionaries\n",
    "    for label in target_labels:\n",
    "        label_data = matching_df[matching_df['label'] == label]\n",
    "        \n",
    "        # Extract and sort unique duration values\n",
    "        unique_dur = sorted(set(label_data['time_diff']))\n",
    "\n",
    "        # Additional code to merge distinct marked start and stop times related to when the seizure starts and ends in each channel for each seizure in one file\n",
    "        durdur = unique_dur\n",
    "\n",
    "        unique_start = sorted(set(label_data['start_time']))\n",
    "        unique_stop = sorted(set(label_data['stop_time']))\n",
    "\n",
    "        if len(unique_start)>0:\n",
    "\n",
    "            merged_start_times = [unique_start[0]]\n",
    "            current_group = unique_start[0]\n",
    "            for i in range(1, len(unique_start)):\n",
    "                if unique_start[i] - current_group <= 3600:\n",
    "                    current_group = unique_start[i]\n",
    "                else:\n",
    "                    merged_start_times.append(current_group)\n",
    "                    current_group = unique_start[i]\n",
    "\n",
    "            merged_stop_times = [unique_stop[-1]]\n",
    "            current_group = unique_stop[-1]\n",
    "            for i in range(len(unique_stop)-1, 0, -1):\n",
    "                if unique_stop[i] - current_group <= 3600:\n",
    "                    current_group = unique_stop[i]\n",
    "                else:\n",
    "                    merged_stop_times.append(current_group)\n",
    "                    current_group = unique_stop[i]\n",
    "\n",
    "            merged_dur = [merged_stop_times[i] - merged_start_times[i] for i in range(len(merged_start_times))]\n",
    "            durdur = merged_dur\n",
    "        #\n",
    "\n",
    "        # if any(dur>300 for dur in unique_dur):\n",
    "        # if sum(unique_dur)>3000:\n",
    "        if any(dur>3000 for dur in durdur):\n",
    "            file_name = \"\".join(file_path)[-22:]\n",
    "            long_sz[f'{label}'].append(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bckg': ['aaaaaatf_s002_t002.csv', 'aaaaabbc_s001_t000.csv', 'aaaaabgs_s001_t001.csv', 'aaaaabpp_s001_t000.csv', 'aaaaacfa_s004_t001.csv', 'aaaaacna_s001_t000.csv', 'aaaaacut_s001_t000.csv', 'aaaaaddu_s001_t000.csv', 'aaaaadhe_s001_t000.csv', 'aaaaadhp_s002_t001.csv', 'aaaaaedo_s001_t000.csv', 'aaaaaent_s001_t002.csv', 'aaaaagro_s003_t000.csv', 'aaaaagxr_s017_t001.csv', 'aaaaaiad_s002_t000.csv', 'aaaaajud_s001_t000.csv', 'aaaaakks_s001_t000.csv', 'aaaaakro_s009_t000.csv', 'aaaaakvb_s001_t000.csv', 'aaaaaliv_s003_t001.csv', 'aaaaamca_s004_t003.csv', 'aaaaamoa_s007_t001.csv', 'aaaaamoe_s002_t004.csv', 'aaaaamoe_s004_t001.csv', 'aaaaamoe_s005_t000.csv', 'aaaaamqy_s007_t004.csv', 'aaaaamrt_s001_t000.csv', 'aaaaaolm_s001_t000.csv', 'aaaaaosa_s008_t007.csv', 'aaaaaota_s002_t000.csv', 'aaaaapay_s002_t000.csv', 'aaaaapcr_s008_t000.csv', 'aaaaapgf_s001_t000.csv', 'aaaaapks_s005_t000.csv', 'aaaaappt_s004_t000.csv', 'aaaaaprc_s003_t000.csv', 'aaaaaqfx_s008_t001.csv', 'aaaaaqjn_s002_t001.csv', 'aaaaaqyo_s004_t003.csv', 'aaaaaqyo_s004_t010.csv', 'aaaaarea_s002_t001.csv', 'aaaaarne_s001_t000.csv', 'aaaaarwg_s001_t000.csv', 'aaaaascj_s002_t000.csv', 'aaaaastr_s002_t002.csv', 'aaaaatem_s003_t006.csv', 'aaaaatki_s002_t001.csv'], 'seiz': [], 'fnsz': ['aaaaaksh_s001_t002.csv'], 'gnsz': ['aaaaadsz_s001_t000.csv', 'aaaaagpk_s009_t000.csv', 'aaaaakkm_s008_t000.csv', 'aaaaalmx_s002_t000.csv', 'aaaaatds_s001_t003.csv'], 'spsz': [], 'cpsz': [], 'absz': [], 'tnsz': [], 'cnsz': [], 'tcsz': [], 'atsz': [], 'mysz': [], 'nesz': []}\n"
     ]
    }
   ],
   "source": [
    "print(long_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "568\n",
      "0\n",
      "27\n",
      "18\n",
      "1\n",
      "8\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for key in long_sz:\n",
    "    print(len(long_sz[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "0\n",
      "9\n",
      "10\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for key in long_sz:\n",
    "    print(len(long_sz[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "0\n",
      "3\n",
      "3\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for key in long_sz:\n",
    "    print(len(long_sz[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eegnetmibci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
